{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = './input/train.csv'\n",
    "PATH_TEST = './input/test.csv'\n",
    "\n",
    "\n",
    "def import_data(path_train, path_test):\n",
    "    '''\n",
    "    Import train data and test data from path.\n",
    "    '''\n",
    "    data_train = pd.read_csv(path_train).as_matrix()\n",
    "    data_test = pd.read_csv(path_test).as_matrix()\n",
    "\n",
    "    random.shuffle(data_train)\n",
    "    random.shuffle(data_test)\n",
    "    \n",
    "    return data_train[:, 1:], data_train[:, 0:1].reshape(data_train.shape[0], 1),\\\n",
    "            data_test[:, 1:], data_test[:, 0:1].reshape(data_test.shape[0], 1)\n",
    "\n",
    "def kernel_initializer(shape):\n",
    "    '''\n",
    "    Initial kernel in convolution layer or weight in full connection layer.\n",
    "    '''\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1, dtype=\"float32\")\n",
    "    return tf.Variable(initial_value=initial)\n",
    "\n",
    "def bias_initializer(shape):\n",
    "    initial = tf.constant(shape=shape, value=1, dtype=\"float32\")\n",
    "    return tf.Variable(initial_value=initial)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, label_train, data_test, label_test = import_data(PATH_TRAIN, PATH_TEST)\n",
    "\n",
    "# Divide to validation data\n",
    "# 42000 = 4200 + 37800\n",
    "data_valid = data_train[0:4200, :]\n",
    "label_valid = label_train[0:4200, :].reshape([4200, 1])\n",
    "\n",
    "data_train = data_train[4200:, :]\n",
    "label_train = label_train[4200:, :].reshape([label_train.shape[0]-4200, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for the trainable variables.\n",
    "\n",
    "# Convolution layer C1 trainable variables.\n",
    "kernel_c1 = kernel_initializer((5, 5, 1, 6))\n",
    "bias_c1 = bias_initializer((6, ))\n",
    "\n",
    "# Subsample layer S2 trainable variables.\n",
    "kernel_s2 = kernel_initializer((6,))\n",
    "bias_s2 = bias_initializer((6,))\n",
    "\n",
    "# Convolution layer C3 trainable variables.\n",
    "kernel_c3 = kernel_initializer((5, 5, 6, 16))\n",
    "bias_c3 = bias_initializer((16, ))\n",
    "\n",
    "# Subsample layer S4 trainable variables.\n",
    "kernel_s4 = kernel_initializer((16,))\n",
    "bias_s4 = bias_initializer((16,))\n",
    "\n",
    "# Convolution layer C5 trainable variables.\n",
    "kernel_c5 = kernel_initializer((4, 4, 16, 120))\n",
    "bias_c5 = bias_initializer((120,))\n",
    "\n",
    "# Full connection layer F6 trainable variables.\n",
    "kernel_f6 = kernel_initializer((1, 1, 120, 84))\n",
    "bias_f6 = bias_initializer((84,))\n",
    "\n",
    "# Full connection layer output trainable variables.\n",
    "kernel_f7 = kernel_initializer((1, 1, 84, 10))\n",
    "bias_f7 = bias_initializer((10,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 420 and 1 for 'SoftmaxCrossEntropyWithLogits_19' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [420,1], [1,420].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 420 and 1 for 'SoftmaxCrossEntropyWithLogits_19' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [420,1], [1,420].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a511068111bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Compute loss and make optimite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m                                               \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0moptimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagradOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0moptimite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1781\u001b[0m   \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1783\u001b[0;31m       precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m   \u001b[0;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   4362\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4363\u001b[0m         \u001b[0;34m\"SoftmaxCrossEntropyWithLogits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4364\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4365\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4366\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 420 and 1 for 'SoftmaxCrossEntropyWithLogits_19' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [420,1], [1,420]."
     ]
    }
   ],
   "source": [
    "## Forward propapation (connect the graph).\n",
    "\n",
    "batch = 420\n",
    "\n",
    "# Make placeholder.\n",
    "X = tf.placeholder(shape=(batch, 28, 28, 1), dtype=\"float32\")\n",
    "Y = tf.placeholder(shape=(1, batch), dtype=\"float32\")\n",
    "\n",
    "# Convolution layer 1, kernel = 5x5x6, output batchx24x24x6\n",
    "X1 = tf.nn.conv2d(X, kernel_c1, strides=[1, 1, 1, 1], padding=\"VALID\")+bias_c1\n",
    "\n",
    "# Subsample layer 2, output batchx12x12x6\n",
    "X2 = tf.nn.avg_pool(X1, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")*kernel_s2+bias_s2\n",
    "\n",
    "# Convolution layer 3, kernel = 5x5x6x16, output batchx8x8x16\n",
    "X3 = tf.nn.conv2d(X2, kernel_c3, strides=[1, 1, 1, 1], padding=\"VALID\")+bias_c3\n",
    "\n",
    "# Subsample layer 4, output batchx4x4x16\n",
    "X4 = tf.nn.avg_pool(X3, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")*kernel_s4+bias_s4\n",
    "\n",
    "# Convolution layer 5, kernel = 4x4x16x160, output batchx1x1x160\n",
    "X5 = tf.nn.conv2d(X4, kernel_c5, strides=[1, 1, 1, 1], padding=\"VALID\")+bias_c5\n",
    "\n",
    "# Full Connection Layer 6, kernel = 1x1x120x84, output batchx1x1x84\n",
    "X6 = tf.sigmoid(tf.nn.conv2d(X5, kernel_f6, strides=[1, 1, 1, 1], padding=\"VALID\")+bias_f6)\n",
    "\n",
    "# Full Connection Layer 7, kernel = 1x1x10x84, output batchx1x1x10\n",
    "X7 = tf.sigmoid(tf.nn.conv2d(X6, kernel_f7, strides=[1, 1, 1, 1], padding=\"VALID\")+bias_f7)\n",
    "\n",
    "# Compute loss and make optimite\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=Y,\\\n",
    "                                               logits=tf.reduce_max(X7, axis=-1))\n",
    "optimiter = tf.train.AdagradOptimizer(learning_rate=0.01)\n",
    "optimite = optimiter.minimize(loss)\n",
    "\n",
    "# Compute accuracy\n",
    "Y_ = tf.arg_max(X7, dimension=1)\n",
    "accuracy = 1-tf.count_nonzero(tf.to_int64(Y)-Y_)/batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "g_init = tf.global_variables_initializer()\n",
    "sess.run(g_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06904761904761902\n",
      "0.08809523809523812\n",
      "0.080952380952381\n",
      "0.08809523809523812\n",
      "0.0976190476190476\n",
      "0.07857142857142863\n",
      "0.09523809523809523\n",
      "0.10238095238095235\n",
      "0.09285714285714286\n",
      "0.07380952380952377\n",
      "0.07380952380952377\n",
      "0.10238095238095235\n",
      "0.06904761904761902\n",
      "0.09285714285714286\n",
      "0.07380952380952377\n",
      "0.0714285714285714\n",
      "0.05714285714285716\n",
      "0.11904761904761907\n",
      "0.09523809523809523\n",
      "0.09047619047619049\n",
      "0.09523809523809523\n",
      "0.08809523809523812\n",
      "0.08571428571428574\n",
      "0.09523809523809523\n",
      "0.080952380952381\n",
      "0.09285714285714286\n",
      "0.11190476190476195\n",
      "0.10238095238095235\n",
      "0.06904761904761902\n",
      "0.080952380952381\n",
      "0.09285714285714286\n",
      "0.09047619047619049\n",
      "0.08333333333333337\n",
      "0.07619047619047614\n",
      "0.07619047619047614\n",
      "0.11190476190476195\n",
      "0.08809523809523812\n",
      "0.08333333333333337\n",
      "0.08809523809523812\n",
      "0.07380952380952377\n",
      "0.08333333333333337\n",
      "0.09285714285714286\n",
      "0.11190476190476195\n",
      "0.08809523809523812\n",
      "0.06190476190476191\n",
      "0.0714285714285714\n",
      "0.10238095238095235\n",
      "0.09285714285714286\n",
      "0.1166666666666667\n",
      "0.0976190476190476\n",
      "0.0976190476190476\n",
      "0.09999999999999998\n",
      "0.08809523809523812\n",
      "0.07380952380952377\n",
      "0.08571428571428574\n",
      "0.080952380952381\n",
      "0.080952380952381\n",
      "0.08809523809523812\n",
      "0.07619047619047614\n",
      "0.10238095238095235\n",
      "0.08809523809523812\n",
      "0.09285714285714286\n",
      "0.09285714285714286\n",
      "0.09047619047619049\n",
      "0.07380952380952377\n",
      "0.1071428571428571\n",
      "0.11190476190476195\n",
      "0.0714285714285714\n",
      "0.09285714285714286\n",
      "0.10952380952380958\n",
      "0.06428571428571428\n",
      "0.0976190476190476\n",
      "0.09523809523809523\n",
      "0.09999999999999998\n",
      "0.07857142857142863\n",
      "0.09523809523809523\n",
      "0.09285714285714286\n",
      "0.0976190476190476\n",
      "0.09523809523809523\n",
      "0.08809523809523812\n",
      "0.09523809523809523\n",
      "0.07380952380952377\n",
      "0.10238095238095235\n",
      "0.08809523809523812\n",
      "0.11428571428571432\n",
      "0.09523809523809523\n",
      "0.12380952380952381\n",
      "0.0976190476190476\n",
      "0.07380952380952377\n",
      "0.10238095238095235\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i in range(round(data_train.shape[0]/batch)):\n",
    "    accuracy_epoch, _ = sess.run([accuracy, optimite], \\\n",
    "                                 feed_dict={X: data_train[i*batch:(i+1)*batch, :].reshape([batch, 28, 28, 1]),\\\n",
    "                                            Y: label_train[i*batch:(i+1)*batch].reshape([1, batch])})\n",
    "    print(accuracy_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
