{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.linear_model as linear_model\n",
    "import math as math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def import_data_train_valid(path_data, train_rate = 0):\n",
    "    if train_rate == 0:\n",
    "        return np.array(pd.read_csv(path_data)), []\n",
    "    data = np.array(pd.read_csv(path_data))\n",
    "    np.random.shuffle(data)\n",
    "    valid_num = round(data.shape[0]*(1-train_rate))\n",
    "    return data[valid_num:, :], data[0:valid_num, :]\n",
    "    \n",
    "def import_data_test(path_data):\n",
    "    data = np.array(pd.read_csv(path_data))\n",
    "    return data\n",
    "\n",
    "def std_scaler(data):\n",
    "    # Standardization scaler\n",
    "    # ? Standardization of train data, but what about validation data or test data.\n",
    "    # ? Can we use the same variance and mean scaling.\n",
    "    return preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "def maxmin_scaler(data):\n",
    "    return preprocessing.MinMaxScaler().fit(data)\n",
    "\n",
    "def corr_analysis(data, threshold = 0.3):\n",
    "    # correlate analysis\n",
    "    # return column need to be remove.\n",
    "    list_r = []\n",
    "    for i in range(data.shape[1]-1):\n",
    "        if (np.std(data[:, i]) != 0):\n",
    "            list_r.append(np.corrcoef(data[:, i], data[:, -1])[0, 1])\n",
    "        else:\n",
    "            list_r.append(0)\n",
    "            \n",
    "    list_r = np.array(list_r)\n",
    "    list_r = preprocessing.minmax_scale(np.abs(list_r))\n",
    "    return np.argwhere(list_r < threshold), np.argwhere(list_r >= threshold)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_t, data_v = import_data_train_valid('./train.csv', 0)\n",
    "\n",
    "# 初始化值很关键，std设置小一点\n",
    "# corr  rate_l      batch  epoch    loss  train\n",
    "# 0.005 0.05-0.01   500                   0.5\n",
    "# 0.05  0.001       1000   10000+   70    0.5\n",
    "# 0.1   0.001       1000   10000+   76\n",
    "# 0.2   0.005       500    100000+  82\n",
    "# 0.3   0.005       1000   10000+   90\n",
    "# 0.4   0.005       1000   10000+   100\n",
    "\n",
    "# Data analysis.\n",
    "# low_col, hig_col = corr_analysis(data_t, 0.9)\n",
    "# data_t = np.delete(data_t, hig_col, axis=1)\n",
    "# data_v = np.delete(data_v, low_col, axis=1)\n",
    "\n",
    "data_train = data_t[:, :-1]\n",
    "label_train = data_t[:, -1]\n",
    "\n",
    "# data_valid = data_v[:, :-1]\n",
    "# label_valid = data_v[:, -1]\n",
    "\n",
    "# Data Square Augmentation\n",
    "# !!!! Error, because the power make data can't use same normalization operation.\n",
    "# data_tmp = data_train\n",
    "# data_tmp = np.column_stack((data_tmp, np.square(data_train)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 3)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 4)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 5)))\n",
    "# data_train = np.column_stack((data_tmp, np.power(data_train, 6)))\n",
    "\n",
    "# preprocess\n",
    "scaler_x = std_scaler(data_train)\n",
    "data_train = scaler_x.transform(data_train)\n",
    "# data_valid = scaler_x.transform(data_valid)\n",
    "\n",
    "# scaler_y = std_scaler(label_train.reshape([-1, 1]))\n",
    "# label_train = scaler_y.transform(label_train.reshape([-1, 1]))\n",
    "# label_valid = scaler_y.transform(label_valid.reshape([-1, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0, loss=2711.641357\n",
      "e=20, loss=2662.885498\n",
      "e=40, loss=2393.313477\n",
      "e=60, loss=112.888992\n",
      "e=80, loss=73.008743\n",
      "e=100, loss=59.849667\n",
      "e=120, loss=52.221420\n",
      "e=140, loss=46.942131\n",
      "e=160, loss=42.954960\n",
      "e=180, loss=39.793091\n",
      "e=200, loss=37.207668\n",
      "e=220, loss=35.039967\n",
      "e=240, loss=33.189442\n",
      "e=260, loss=31.586132\n",
      "e=280, loss=30.179987\n",
      "e=300, loss=28.938543\n",
      "e=320, loss=27.829044\n",
      "e=340, loss=26.828127\n",
      "e=360, loss=25.919552\n",
      "e=380, loss=25.090355\n",
      "e=400, loss=24.326281\n",
      "e=420, loss=23.618444\n",
      "e=440, loss=22.959061\n",
      "e=460, loss=22.341358\n",
      "e=480, loss=21.762720\n",
      "e=500, loss=21.216232\n",
      "e=520, loss=20.698669\n",
      "e=540, loss=20.207344\n",
      "e=560, loss=19.740063\n",
      "e=580, loss=19.294580\n",
      "e=600, loss=18.869343\n",
      "e=620, loss=18.462709\n",
      "e=640, loss=18.073013\n",
      "e=660, loss=17.699020\n",
      "e=680, loss=17.340200\n",
      "e=700, loss=16.995098\n",
      "e=720, loss=16.662645\n",
      "e=740, loss=16.342123\n",
      "e=760, loss=16.032278\n",
      "e=780, loss=15.732926\n",
      "e=800, loss=15.443254\n",
      "e=820, loss=15.162377\n",
      "e=840, loss=14.889989\n",
      "e=860, loss=14.625813\n",
      "e=880, loss=14.369395\n",
      "e=900, loss=14.120429\n",
      "e=920, loss=13.878613\n",
      "e=940, loss=13.643076\n",
      "e=960, loss=13.413719\n",
      "e=980, loss=13.190575\n",
      "e=1000, loss=12.972857\n",
      "e=1020, loss=12.760862\n",
      "e=1040, loss=12.554503\n",
      "e=1060, loss=12.353301\n",
      "e=1080, loss=12.157090\n",
      "e=1100, loss=11.965643\n",
      "e=1120, loss=11.778881\n",
      "e=1140, loss=11.596415\n",
      "e=1160, loss=11.418021\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # full connection layer\n",
    "        self.fc1 = nn.Linear(385, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 100)\n",
    "        self.fc6 = nn.Linear(100, 50)\n",
    "        self.fc7 = nn.Linear(50, 50)\n",
    "        self.fc8 = nn.Linear(50, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward network\n",
    "        r1 = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(r1))\n",
    "        r2 = F.relu(self.fc3(x+r1))\n",
    "        x = F.relu(self.fc4(r2))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        r3 = F.relu(self.fc6(x+r2))\n",
    "        x = F.relu(self.fc7(r3))\n",
    "        return self.fc8(x+r3)\n",
    "    \n",
    "    def loss(self, input_t, target):\n",
    "        out = self.forward(input_t)\n",
    "        criter = nn.MSELoss()\n",
    "        loss = criter(out, target)\n",
    "        return loss, loss.data.numpy()\n",
    "    \n",
    "    def optim(self, input_t, target, lr):\n",
    "        optimiter = optim.SGD(self.parameters(), lr = 0.0001)\n",
    "        loss, _ = self.loss(input_t, target)\n",
    "        optimiter.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiter.step()\n",
    "        \n",
    "\n",
    "# initial input and label variable\n",
    "input_t = Variable(torch.Tensor(data_train))\n",
    "target =Variable(torch.Tensor(label_train))\n",
    "    \n",
    "# calculate loss\n",
    "# out = net.forward(input_t)\n",
    "# criter = nn.MSELoss()\n",
    "# loss = criter(out, target)\n",
    "# print(scaler_y.inverse_transform(loss.data.numpy()))\n",
    "\n",
    "# print(net.fc1.bias.grad)\n",
    "\n",
    "\n",
    "# Net instance\n",
    "net = Net()\n",
    "\n",
    "# update with a learning rate\n",
    "rate_l = 0.0005\n",
    "epoch = 100000\n",
    "sepoch = 20\n",
    "for e in range(epoch):\n",
    "    # backward(): calculate every trainable variable's grad\n",
    "    net.optim(input_t, target, rate_l)\n",
    "    if (e%sepoch == 0):\n",
    "        _, loss_p = net.loss(input_t, target)\n",
    "        print(\"e=%d, loss=%f\"%(e, loss_p))\n",
    "        if (loss_p < 1):\n",
    "            rate_l = 0.0001\n",
    "            sepoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data_test = import_data_test(\"./test.csv\")\n",
    "\n",
    "#data_test = np.delete(data_test, low_col, axis=1)\n",
    "data_test = scaler_x.transform(data_test)\n",
    "y_ = net.forward(Variable(torch.Tensor(data_test)))\n",
    "\n",
    "f = open(\"./predict0.3.csv\", \"w\")\n",
    "f.write(\"id,reference\\n\")\n",
    "for i in range(y_.shape[0]):\n",
    "    f.write(\"%d, %.8f\\n\"%(i, y_.data[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgr = linear_model.LinearRegression()\n",
    "# rgr = linear_model.SGDRegressor(eta0 = 0.0001, verbose=1, max_iter = 100)\n",
    "rgr.fit(data_train, np.squeeze(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.37325186156326\n"
     ]
    }
   ],
   "source": [
    "# average squared loss\n",
    "print(mean_squared_error(scaler_y.inverse_transform(rgr.predict(data_train)),\\\n",
    "                         scaler_y.inverse_transform(label_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data_test = import_data_test(\"./test.csv\")\n",
    "\n",
    "#data_test = np.delete(data_test, low_col, axis=1)\n",
    "data_test = scaler_x.transform(data_test)\n",
    "y_ = scaler_y.inverse_transform(rgr.predict(data_test))\n",
    "\n",
    "f = open(\"./predict.csv\", \"w\")\n",
    "f.write(\"id,reference\\n\")\n",
    "for i in range(y_.shape[0]):\n",
    "    f.write(\"%d, %.8f\\n\"%(i, y_[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My SGD\n",
    "Following algorithm is my SGD regressor, but it is no well that it can only reach mean squared loss of 67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial train params\n",
    "w = np.random.normal(loc=-0.5, scale=0.001, size=data_train.shape[1])\n",
    "b = label_train.mean()\n",
    "\n",
    "\n",
    "batch = 5000\n",
    "#rate_l = 0.06\n",
    "rate_l = 0.006\n",
    "epoch = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train loss=439.035438, valid loss=478.316155\n",
      "[1000] train loss=68.266391, valid loss=70.480676\n",
      "[2000] train loss=68.218196, valid loss=70.451167\n",
      "[3000] train loss=68.207164, valid loss=70.436242\n",
      "[4000] train loss=68.202488, valid loss=70.428358\n",
      "[5000] train loss=68.200030, valid loss=70.423896\n",
      "[6000] train loss=68.198622, valid loss=70.421222\n",
      "[7000] train loss=68.197777, valid loss=70.419549\n",
      "[8000] train loss=68.197252, valid loss=70.418471\n",
      "[9000] train loss=68.196915, valid loss=70.417763\n",
      "[10000] train loss=68.196692, valid loss=70.417293\n",
      "[11000] train loss=68.196540, valid loss=70.416979\n",
      "[12000] train loss=68.196433, valid loss=70.416768\n",
      "[13000] train loss=68.196356, valid loss=70.416627\n",
      "[14000] train loss=68.196298, valid loss=70.416533\n",
      "[15000] train loss=68.196253, valid loss=70.416471\n",
      "[16000] train loss=68.196217, valid loss=70.416431\n",
      "[17000] train loss=68.196188, valid loss=70.416405\n",
      "[18000] train loss=68.196164, valid loss=70.416389\n",
      "[19000] train loss=68.196143, valid loss=70.416379\n",
      "[20000] train loss=68.196124, valid loss=70.416374\n",
      "[21000] train loss=68.196108, valid loss=70.416372\n",
      "[22000] train loss=68.196093, valid loss=70.416371\n",
      "[23000] train loss=68.196079, valid loss=70.416371\n",
      "[24000] train loss=68.196066, valid loss=70.416372\n",
      "[25000] train loss=68.196055, valid loss=70.416373\n",
      "[26000] train loss=68.196043, valid loss=70.416374\n",
      "[27000] train loss=68.196033, valid loss=70.416375\n",
      "[28000] train loss=68.196023, valid loss=70.416376\n",
      "[29000] train loss=68.196013, valid loss=70.416377\n",
      "[30000] train loss=68.196004, valid loss=70.416377\n",
      "[31000] train loss=68.195995, valid loss=70.416377\n",
      "[32000] train loss=68.195987, valid loss=70.416377\n",
      "[33000] train loss=68.195979, valid loss=70.416376\n",
      "[34000] train loss=68.195971, valid loss=70.416375\n",
      "[35000] train loss=68.195963, valid loss=70.416374\n",
      "[36000] train loss=68.195956, valid loss=70.416373\n",
      "[37000] train loss=68.195948, valid loss=70.416372\n",
      "[38000] train loss=68.195942, valid loss=70.416371\n",
      "[39000] train loss=68.195935, valid loss=70.416369\n",
      "[40000] train loss=68.195928, valid loss=70.416368\n",
      "[41000] train loss=68.195922, valid loss=70.416366\n",
      "[42000] train loss=68.195916, valid loss=70.416364\n",
      "[43000] train loss=68.195910, valid loss=70.416362\n",
      "[44000] train loss=68.195904, valid loss=70.416360\n",
      "[45000] train loss=68.195898, valid loss=70.416358\n",
      "[46000] train loss=68.195892, valid loss=70.416356\n",
      "[47000] train loss=68.195887, valid loss=70.416354\n",
      "[48000] train loss=68.195881, valid loss=70.416352\n",
      "[49000] train loss=68.195876, valid loss=70.416350\n",
      "[50000] train loss=68.195871, valid loss=70.416348\n",
      "[51000] train loss=68.195866, valid loss=70.416346\n",
      "[52000] train loss=68.195861, valid loss=70.416344\n",
      "[53000] train loss=68.195856, valid loss=70.416341\n",
      "[54000] train loss=68.195852, valid loss=70.416339\n",
      "[55000] train loss=68.195847, valid loss=70.416337\n",
      "[56000] train loss=68.195842, valid loss=70.416335\n",
      "[57000] train loss=68.195838, valid loss=70.416333\n",
      "[58000] train loss=68.195834, valid loss=70.416331\n",
      "[59000] train loss=68.195829, valid loss=70.416329\n",
      "[60000] train loss=68.195825, valid loss=70.416326\n",
      "[61000] train loss=68.195821, valid loss=70.416324\n",
      "[62000] train loss=68.195817, valid loss=70.416322\n",
      "[63000] train loss=68.195813, valid loss=70.416320\n",
      "[64000] train loss=68.195809, valid loss=70.416318\n",
      "[65000] train loss=68.195806, valid loss=70.416316\n",
      "[66000] train loss=68.195802, valid loss=70.416314\n",
      "[67000] train loss=68.195798, valid loss=70.416312\n",
      "[68000] train loss=68.195795, valid loss=70.416310\n",
      "[69000] train loss=68.195791, valid loss=70.416308\n",
      "[70000] train loss=68.195788, valid loss=70.416306\n",
      "[71000] train loss=68.195784, valid loss=70.416304\n",
      "[72000] train loss=68.195781, valid loss=70.416303\n",
      "[73000] train loss=68.195778, valid loss=70.416301\n",
      "[74000] train loss=68.195775, valid loss=70.416299\n",
      "[75000] train loss=68.195772, valid loss=70.416297\n",
      "[76000] train loss=68.195769, valid loss=70.416295\n",
      "[77000] train loss=68.195766, valid loss=70.416294\n",
      "[78000] train loss=68.195763, valid loss=70.416292\n",
      "[79000] train loss=68.195760, valid loss=70.416290\n",
      "[80000] train loss=68.195757, valid loss=70.416288\n",
      "[81000] train loss=68.195754, valid loss=70.416287\n",
      "[82000] train loss=68.195751, valid loss=70.416285\n",
      "[83000] train loss=68.195749, valid loss=70.416284\n",
      "[84000] train loss=68.195746, valid loss=70.416282\n",
      "[85000] train loss=68.195743, valid loss=70.416280\n",
      "[86000] train loss=68.195741, valid loss=70.416279\n",
      "[87000] train loss=68.195738, valid loss=70.416277\n",
      "[88000] train loss=68.195736, valid loss=70.416276\n",
      "[89000] train loss=68.195733, valid loss=70.416274\n",
      "[90000] train loss=68.195731, valid loss=70.416273\n",
      "[91000] train loss=68.195729, valid loss=70.416271\n",
      "[92000] train loss=68.195726, valid loss=70.416270\n",
      "[93000] train loss=68.195724, valid loss=70.416269\n",
      "[94000] train loss=68.195722, valid loss=70.416267\n",
      "[95000] train loss=68.195720, valid loss=70.416266\n",
      "[96000] train loss=68.195718, valid loss=70.416265\n",
      "[97000] train loss=68.195715, valid loss=70.416263\n",
      "[98000] train loss=68.195713, valid loss=70.416262\n",
      "[99000] train loss=68.195711, valid loss=70.416261\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "losst = 0\n",
    "lossv = 0\n",
    "for e in range(epoch):\n",
    "    \n",
    "    for bat in range(math.ceil(data_train.shape[0]/batch)):\n",
    "        front = bat*batch\n",
    "        end = (bat+1)*batch\n",
    "        if (end > data_train.shape[0]):\n",
    "            end = data_train.shape[0]\n",
    "        \n",
    "        data_batch = data_train[front:end, :]\n",
    "        y_ = np.matmul(data_batch, w)+b\n",
    "        \n",
    "        dif_y = y_-label_train[front:end]\n",
    "        dif_w = np.mean(data_batch*dif_y.reshape(dif_y.shape[0], 1), axis=0)\n",
    "        dif_b = dif_y.mean()\n",
    "        \n",
    "        # update\n",
    "        w = w-dif_w*rate_l\n",
    "        b = b-dif_b*rate_l\n",
    "    \n",
    "    # compute correct in train and valid\n",
    "    \n",
    "    # train\n",
    "    if (e%1000==0):\n",
    "        y_t = np.matmul(data_train, w)+b\n",
    "        losst = np.mean(np.square(y_t-label_train))\n",
    "\n",
    "        # valid\n",
    "        y_v = np.matmul(data_valid, w)+b\n",
    "        lossv = np.mean(np.square(y_v-label_valid))\n",
    "\n",
    "        print(\"[%d] train loss=%f, valid loss=%f\"\\\n",
    "              %(e, losst, lossv))\n",
    "        \n",
    "    if e%1000 == 0:\n",
    "        if lossv < 10:\n",
    "            rate_l = 0.001\n",
    "        elif lossv < 30:\n",
    "            rate_l = 0.002\n",
    "        elif lossv < 50:\n",
    "            rate_l = 0.003\n",
    "        elif lossv < 70:\n",
    "            rate_l = 0.004\n",
    "        elif lossv < 90:\n",
    "            rate_l = 0.005\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11250, 375)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
