{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.linear_model as linear_model\n",
    "import math as math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def import_data_train_valid(path_data, train_rate = 0):\n",
    "    if train_rate == 0:\n",
    "        return np.array(pd.read_csv(path_data)), []\n",
    "    data = np.array(pd.read_csv(path_data))\n",
    "    np.random.shuffle(data)\n",
    "    valid_num = round(data.shape[0]*(1-train_rate))\n",
    "    return data[valid_num:, :], data[0:valid_num, :]\n",
    "    \n",
    "def import_data_test(path_data):\n",
    "    data = np.array(pd.read_csv(path_data))\n",
    "    return data\n",
    "\n",
    "def std_scaler(data):\n",
    "    # Standardization scaler\n",
    "    # ? Standardization of train data, but what about validation data or test data.\n",
    "    # ? Can we use the same variance and mean scaling.\n",
    "    return preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "def maxmin_scaler(data):\n",
    "    return preprocessing.MinMaxScaler().fit(data)\n",
    "\n",
    "def corr_analysis(data, threshold = 0.3):\n",
    "    # correlate analysis\n",
    "    # return column need to be remove.\n",
    "    list_r = []\n",
    "    for i in range(data.shape[1]-1):\n",
    "        if (np.std(data[:, i]) != 0):\n",
    "            list_r.append(np.corrcoef(data[:, i], data[:, -1])[0, 1])\n",
    "        else:\n",
    "            list_r.append(0)\n",
    "            \n",
    "    list_r = np.array(list_r)\n",
    "    list_r = preprocessing.minmax_scale(np.abs(list_r))\n",
    "    return np.argwhere(list_r < threshold), np.argwhere(list_r >= threshold)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_t, data_v = import_data_train_valid('./train.csv', 0)\n",
    "\n",
    "# 初始化值很关键，std设置小一点\n",
    "# corr  rate_l      batch  epoch    loss  train\n",
    "# 0.005 0.05-0.01   500                   0.5\n",
    "# 0.05  0.001       1000   10000+   70    0.5\n",
    "# 0.1   0.001       1000   10000+   76\n",
    "# 0.2   0.005       500    100000+  82\n",
    "# 0.3   0.005       1000   10000+   90\n",
    "# 0.4   0.005       1000   10000+   100\n",
    "\n",
    "# Data analysis.\n",
    "# low_col, hig_col = corr_analysis(data_t, 0.9)\n",
    "# data_t = np.delete(data_t, hig_col, axis=1)\n",
    "# data_v = np.delete(data_v, low_col, axis=1)\n",
    "\n",
    "data_train = data_t[:, :-1]\n",
    "label_train = data_t[:, -1]\n",
    "\n",
    "# data_valid = data_v[:, :-1]\n",
    "# label_valid = data_v[:, -1]\n",
    "\n",
    "# Data Square Augmentation\n",
    "# !!!! Error, because the power make data can't use same normalization operation.\n",
    "# data_tmp = data_train\n",
    "# data_tmp = np.column_stack((data_tmp, np.square(data_train)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 3)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 4)))\n",
    "# data_tmp = np.column_stack((data_tmp, np.power(data_train, 5)))\n",
    "# data_train = np.column_stack((data_tmp, np.power(data_train, 6)))\n",
    "\n",
    "# preprocess\n",
    "scaler_x = std_scaler(data_train)\n",
    "data_train = scaler_x.transform(data_train)\n",
    "# data_valid = scaler_x.transform(data_valid)\n",
    "\n",
    "# scaler_y = std_scaler(label_train.reshape([-1, 1]))\n",
    "# label_train = scaler_y.transform(label_train.reshape([-1, 1]))\n",
    "# label_valid = scaler_y.transform(label_valid.reshape([-1, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 385)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0, loss=2714.126465\n",
      "e=20, loss=2693.652100\n",
      "e=40, loss=2670.303711\n",
      "e=60, loss=2641.147705\n",
      "e=80, loss=2596.912598\n",
      "e=100, loss=2490.516602\n",
      "e=120, loss=1703.023193\n",
      "e=140, loss=133.628372\n",
      "e=160, loss=94.022148\n",
      "e=180, loss=77.027214\n",
      "e=200, loss=66.789688\n",
      "e=220, loss=59.807274\n",
      "e=240, loss=54.679787\n",
      "e=260, loss=50.726181\n",
      "e=280, loss=47.558010\n",
      "e=300, loss=44.945694\n",
      "e=320, loss=42.741539\n",
      "e=340, loss=40.840488\n",
      "e=360, loss=39.176876\n",
      "e=380, loss=37.704506\n",
      "e=400, loss=36.394539\n",
      "e=420, loss=35.218410\n",
      "e=440, loss=34.155384\n",
      "e=460, loss=33.186451\n",
      "e=480, loss=32.299091\n",
      "e=500, loss=31.482109\n",
      "e=520, loss=30.726942\n",
      "e=540, loss=30.025682\n",
      "e=560, loss=29.371080\n",
      "e=580, loss=28.759390\n",
      "e=600, loss=28.185619\n",
      "e=620, loss=27.647104\n",
      "e=640, loss=27.140078\n",
      "e=660, loss=26.660978\n",
      "e=680, loss=26.206598\n",
      "e=700, loss=25.774670\n",
      "e=720, loss=25.363317\n",
      "e=740, loss=24.971186\n",
      "e=760, loss=24.596832\n",
      "e=780, loss=24.237753\n",
      "e=800, loss=23.893085\n",
      "e=820, loss=23.561714\n",
      "e=840, loss=23.243244\n",
      "e=860, loss=22.936800\n",
      "e=880, loss=22.641338\n",
      "e=900, loss=22.356243\n",
      "e=920, loss=22.080870\n",
      "e=940, loss=21.814615\n",
      "e=960, loss=21.556822\n",
      "e=980, loss=21.307045\n",
      "e=1000, loss=21.064857\n",
      "e=1020, loss=20.829882\n",
      "e=1040, loss=20.601494\n",
      "e=1060, loss=20.379263\n",
      "e=1080, loss=20.162846\n",
      "e=1100, loss=19.951990\n",
      "e=1120, loss=19.746796\n",
      "e=1140, loss=19.546471\n",
      "e=1160, loss=19.351006\n",
      "e=1180, loss=19.159883\n",
      "e=1200, loss=18.973518\n",
      "e=1220, loss=18.791267\n",
      "e=1240, loss=18.613420\n",
      "e=1260, loss=18.439499\n",
      "e=1280, loss=18.269262\n",
      "e=1300, loss=18.102612\n",
      "e=1320, loss=17.939394\n",
      "e=1340, loss=17.779268\n",
      "e=1360, loss=17.622339\n",
      "e=1380, loss=17.468546\n",
      "e=1400, loss=17.317551\n",
      "e=1420, loss=17.169165\n",
      "e=1440, loss=17.023079\n",
      "e=1460, loss=16.879274\n",
      "e=1480, loss=16.738066\n",
      "e=1500, loss=16.599068\n",
      "e=1520, loss=16.462339\n",
      "e=1540, loss=16.327726\n",
      "e=1560, loss=16.195274\n",
      "e=1580, loss=16.064838\n",
      "e=1600, loss=15.936522\n",
      "e=1620, loss=15.810030\n",
      "e=1640, loss=15.685372\n",
      "e=1660, loss=15.562572\n",
      "e=1680, loss=15.441679\n",
      "e=1700, loss=15.322590\n",
      "e=1720, loss=15.205032\n",
      "e=1740, loss=15.088960\n",
      "e=1760, loss=14.974260\n",
      "e=1780, loss=14.861134\n",
      "e=1800, loss=14.749673\n",
      "e=1820, loss=14.639667\n",
      "e=1840, loss=14.531223\n",
      "e=1860, loss=14.424161\n",
      "e=1880, loss=14.318288\n",
      "e=1900, loss=14.213618\n",
      "e=1920, loss=14.110160\n",
      "e=1940, loss=14.007880\n",
      "e=1960, loss=13.906770\n",
      "e=1980, loss=13.807114\n",
      "e=2000, loss=13.708677\n",
      "e=2020, loss=13.611547\n",
      "e=2040, loss=13.515453\n",
      "e=2060, loss=13.420471\n",
      "e=2080, loss=13.326669\n",
      "e=2100, loss=13.233843\n",
      "e=2120, loss=13.142091\n",
      "e=2140, loss=13.051566\n",
      "e=2160, loss=12.961875\n",
      "e=2180, loss=12.873081\n",
      "e=2200, loss=12.785368\n",
      "e=2220, loss=12.698467\n",
      "e=2240, loss=12.612622\n",
      "e=2260, loss=12.527730\n",
      "e=2280, loss=12.443810\n",
      "e=2300, loss=12.361023\n",
      "e=2320, loss=12.279195\n",
      "e=2340, loss=12.198103\n",
      "e=2360, loss=12.117887\n",
      "e=2380, loss=12.038484\n",
      "e=2400, loss=11.959804\n",
      "e=2420, loss=11.881860\n",
      "e=2440, loss=11.804599\n",
      "e=2460, loss=11.728079\n",
      "e=2480, loss=11.652267\n",
      "e=2500, loss=11.577315\n",
      "e=2520, loss=11.503058\n",
      "e=2540, loss=11.429482\n",
      "e=2560, loss=11.356647\n",
      "e=2580, loss=11.284638\n",
      "e=2600, loss=11.213151\n",
      "e=2620, loss=11.142356\n",
      "e=2640, loss=11.072104\n",
      "e=2660, loss=11.002459\n",
      "e=2680, loss=10.933518\n",
      "e=2700, loss=10.865280\n",
      "e=2720, loss=10.797584\n",
      "e=2740, loss=10.730542\n",
      "e=2760, loss=10.664147\n",
      "e=2780, loss=10.598380\n",
      "e=2800, loss=10.533358\n",
      "e=2820, loss=10.468847\n",
      "e=2840, loss=10.404987\n",
      "e=2860, loss=10.341702\n",
      "e=2880, loss=10.278922\n",
      "e=2900, loss=10.216816\n",
      "e=2920, loss=10.155403\n",
      "e=2940, loss=10.094442\n",
      "e=2960, loss=10.034176\n",
      "e=2980, loss=9.974353\n",
      "e=3000, loss=9.915129\n",
      "e=3020, loss=9.856440\n",
      "e=3040, loss=9.798162\n",
      "e=3060, loss=9.740493\n",
      "e=3080, loss=9.683252\n",
      "e=3100, loss=9.626535\n",
      "e=3120, loss=9.570374\n",
      "e=3140, loss=9.514731\n",
      "e=3160, loss=9.459597\n",
      "e=3180, loss=9.404958\n",
      "e=3200, loss=9.350874\n",
      "e=3220, loss=9.297320\n",
      "e=3240, loss=9.244233\n",
      "e=3260, loss=9.191554\n",
      "e=3280, loss=9.139407\n",
      "e=3300, loss=9.087661\n",
      "e=3320, loss=9.036309\n",
      "e=3340, loss=8.985368\n",
      "e=3360, loss=8.935022\n",
      "e=3380, loss=8.884980\n",
      "e=3400, loss=8.835340\n",
      "e=3420, loss=8.786221\n",
      "e=3440, loss=8.737329\n",
      "e=3460, loss=8.688812\n",
      "e=3480, loss=8.640692\n",
      "e=3500, loss=8.593007\n",
      "e=3520, loss=8.545797\n",
      "e=3540, loss=8.499006\n",
      "e=3560, loss=8.452564\n",
      "e=3580, loss=8.406545\n",
      "e=3600, loss=8.360890\n",
      "e=3620, loss=8.315691\n",
      "e=3640, loss=8.270885\n",
      "e=3660, loss=8.226459\n",
      "e=3680, loss=8.182277\n",
      "e=3700, loss=8.138461\n",
      "e=3720, loss=8.094948\n",
      "e=3740, loss=8.051758\n",
      "e=3760, loss=8.008894\n",
      "e=3780, loss=7.966338\n",
      "e=3800, loss=7.924146\n",
      "e=3820, loss=7.882285\n",
      "e=3840, loss=7.840733\n",
      "e=3860, loss=7.799460\n",
      "e=3880, loss=7.758528\n",
      "e=3900, loss=7.717895\n",
      "e=3920, loss=7.677559\n",
      "e=3940, loss=7.637556\n",
      "e=3960, loss=7.597827\n",
      "e=3980, loss=7.558305\n",
      "e=4000, loss=7.519232\n",
      "e=4020, loss=7.480454\n",
      "e=4040, loss=7.441997\n",
      "e=4060, loss=7.403821\n",
      "e=4080, loss=7.365975\n",
      "e=4100, loss=7.328466\n",
      "e=4120, loss=7.291166\n",
      "e=4140, loss=7.254248\n",
      "e=4160, loss=7.217507\n",
      "e=4180, loss=7.181199\n",
      "e=4200, loss=7.145059\n",
      "e=4220, loss=7.109226\n",
      "e=4240, loss=7.073602\n",
      "e=4260, loss=7.038189\n",
      "e=4280, loss=7.002921\n",
      "e=4300, loss=6.967986\n",
      "e=4320, loss=6.933224\n",
      "e=4340, loss=6.898829\n",
      "e=4360, loss=6.864675\n",
      "e=4380, loss=6.830746\n",
      "e=4400, loss=6.797073\n",
      "e=4420, loss=6.763605\n",
      "e=4440, loss=6.730438\n",
      "e=4460, loss=6.697517\n",
      "e=4480, loss=6.664785\n",
      "e=4500, loss=6.632362\n",
      "e=4520, loss=6.600223\n",
      "e=4540, loss=6.568361\n",
      "e=4560, loss=6.536747\n",
      "e=4580, loss=6.505306\n",
      "e=4600, loss=6.474172\n",
      "e=4620, loss=6.443324\n",
      "e=4640, loss=6.412800\n",
      "e=4660, loss=6.382473\n",
      "e=4680, loss=6.352304\n",
      "e=4700, loss=6.322358\n",
      "e=4720, loss=6.292681\n",
      "e=4740, loss=6.263251\n",
      "e=4760, loss=6.234030\n",
      "e=4780, loss=6.205057\n",
      "e=4800, loss=6.176235\n",
      "e=4820, loss=6.147599\n",
      "e=4840, loss=6.119090\n",
      "e=4860, loss=6.090882\n",
      "e=4880, loss=6.062855\n",
      "e=4900, loss=6.035076\n",
      "e=4920, loss=6.007461\n",
      "e=4940, loss=5.980087\n",
      "e=4960, loss=5.952862\n",
      "e=4980, loss=5.925896\n",
      "e=5000, loss=5.899069\n",
      "e=5020, loss=5.872552\n",
      "e=5040, loss=5.846172\n",
      "e=5060, loss=5.819963\n",
      "e=5080, loss=5.793931\n",
      "e=5100, loss=5.768079\n",
      "e=5120, loss=5.742440\n",
      "e=5140, loss=5.716952\n",
      "e=5160, loss=5.691661\n",
      "e=5180, loss=5.666623\n",
      "e=5200, loss=5.641732\n",
      "e=5220, loss=5.617099\n",
      "e=5240, loss=5.592675\n",
      "e=5260, loss=5.568413\n",
      "e=5280, loss=5.544318\n",
      "e=5300, loss=5.520414\n",
      "e=5320, loss=5.496656\n",
      "e=5340, loss=5.473082\n",
      "e=5360, loss=5.449685\n",
      "e=5380, loss=5.426419\n",
      "e=5400, loss=5.403342\n",
      "e=5420, loss=5.380445\n",
      "e=5440, loss=5.357701\n",
      "e=5460, loss=5.335159\n",
      "e=5480, loss=5.312781\n",
      "e=5500, loss=5.290588\n",
      "e=5520, loss=5.268556\n",
      "e=5540, loss=5.246681\n",
      "e=5560, loss=5.224971\n",
      "e=5580, loss=5.203365\n",
      "e=5600, loss=5.181922\n",
      "e=5620, loss=5.160631\n",
      "e=5640, loss=5.139449\n",
      "e=5660, loss=5.118436\n",
      "e=5680, loss=5.097613\n",
      "e=5700, loss=5.076945\n",
      "e=5720, loss=5.056404\n",
      "e=5740, loss=5.036036\n",
      "e=5760, loss=5.015893\n",
      "e=5780, loss=4.995855\n",
      "e=5800, loss=4.975964\n",
      "e=5820, loss=4.956241\n",
      "e=5840, loss=4.936578\n",
      "e=5860, loss=4.917059\n",
      "e=5880, loss=4.897665\n",
      "e=5900, loss=4.878332\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # full connection layer\n",
    "        self.fc1 = nn.Linear(385, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.fc4 = nn.Linear(25, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward network\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "    def loss(self, input_t, target):\n",
    "        out = self.forward(input_t)\n",
    "        criter = nn.MSELoss()\n",
    "        loss = criter(out, target)\n",
    "        return loss, loss.data.numpy()\n",
    "    \n",
    "    def optim(self, input_t, target):\n",
    "        optimiter = optim.SGD(self.parameters(), lr = 0.0001)\n",
    "        loss, _ = self.loss(input_t, target)\n",
    "        self.zero_grad()\n",
    "        loss.backward(retrain_graph=True)\n",
    "        optimiter.step()\n",
    "        return loss(input_t, target)\n",
    "        \n",
    "\n",
    "# initial input and label variable\n",
    "input_t = Variable(torch.Tensor(data_train))\n",
    "target =Variable(torch.Tensor(label_train))\n",
    "    \n",
    "# calculate loss\n",
    "# out = net.forward(input_t)\n",
    "# criter = nn.MSELoss()\n",
    "# loss = criter(out, target)\n",
    "# print(scaler_y.inverse_transform(loss.data.numpy()))\n",
    "\n",
    "# print(net.fc1.bias.grad)\n",
    "\n",
    "\n",
    "# Net instance\n",
    "net = Net()\n",
    "\n",
    "# update with a learning rate\n",
    "rate_l = 0.0001\n",
    "epoch = 10000\n",
    "for e in range(epoch):\n",
    "    # backward(): calculate every trainable variable's grad\n",
    "    loss, loss_print = net.loss(input_t, target)\n",
    "    if (e%20 == 0):\n",
    "        print(\"e=%d, loss=%f\"%(e, loss_print))\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    for p in net.parameters():\n",
    "        p.data.sub_(rate_l*p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data_test = import_data_test(\"./test.csv\")\n",
    "\n",
    "#data_test = np.delete(data_test, low_col, axis=1)\n",
    "data_test = scaler_x.transform(data_test)\n",
    "y_ = net.forward(Variable(torch.Tensor(data_test)))\n",
    "\n",
    "f = open(\"./predict.csv\", \"w\")\n",
    "f.write(\"id,reference\\n\")\n",
    "for i in range(y_.shape[0]):\n",
    "    f.write(\"%d, %.8f\\n\"%(i, y_.data[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgr = linear_model.LinearRegression()\n",
    "# rgr = linear_model.SGDRegressor(eta0 = 0.0001, verbose=1, max_iter = 100)\n",
    "rgr.fit(data_train, np.squeeze(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.37325186156326\n"
     ]
    }
   ],
   "source": [
    "# average squared loss\n",
    "print(mean_squared_error(scaler_y.inverse_transform(rgr.predict(data_train)),\\\n",
    "                         scaler_y.inverse_transform(label_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data_test = import_data_test(\"./test.csv\")\n",
    "\n",
    "#data_test = np.delete(data_test, low_col, axis=1)\n",
    "data_test = scaler_x.transform(data_test)\n",
    "y_ = scaler_y.inverse_transform(rgr.predict(data_test))\n",
    "\n",
    "f = open(\"./predict.csv\", \"w\")\n",
    "f.write(\"id,reference\\n\")\n",
    "for i in range(y_.shape[0]):\n",
    "    f.write(\"%d, %.8f\\n\"%(i, y_[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My SGD\n",
    "Following algorithm is my SGD regressor, but it is no well that it can only reach mean squared loss of 67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial train params\n",
    "w = np.random.normal(loc=-0.5, scale=0.001, size=data_train.shape[1])\n",
    "b = label_train.mean()\n",
    "\n",
    "\n",
    "batch = 5000\n",
    "#rate_l = 0.06\n",
    "rate_l = 0.006\n",
    "epoch = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train loss=439.035438, valid loss=478.316155\n",
      "[1000] train loss=68.266391, valid loss=70.480676\n",
      "[2000] train loss=68.218196, valid loss=70.451167\n",
      "[3000] train loss=68.207164, valid loss=70.436242\n",
      "[4000] train loss=68.202488, valid loss=70.428358\n",
      "[5000] train loss=68.200030, valid loss=70.423896\n",
      "[6000] train loss=68.198622, valid loss=70.421222\n",
      "[7000] train loss=68.197777, valid loss=70.419549\n",
      "[8000] train loss=68.197252, valid loss=70.418471\n",
      "[9000] train loss=68.196915, valid loss=70.417763\n",
      "[10000] train loss=68.196692, valid loss=70.417293\n",
      "[11000] train loss=68.196540, valid loss=70.416979\n",
      "[12000] train loss=68.196433, valid loss=70.416768\n",
      "[13000] train loss=68.196356, valid loss=70.416627\n",
      "[14000] train loss=68.196298, valid loss=70.416533\n",
      "[15000] train loss=68.196253, valid loss=70.416471\n",
      "[16000] train loss=68.196217, valid loss=70.416431\n",
      "[17000] train loss=68.196188, valid loss=70.416405\n",
      "[18000] train loss=68.196164, valid loss=70.416389\n",
      "[19000] train loss=68.196143, valid loss=70.416379\n",
      "[20000] train loss=68.196124, valid loss=70.416374\n",
      "[21000] train loss=68.196108, valid loss=70.416372\n",
      "[22000] train loss=68.196093, valid loss=70.416371\n",
      "[23000] train loss=68.196079, valid loss=70.416371\n",
      "[24000] train loss=68.196066, valid loss=70.416372\n",
      "[25000] train loss=68.196055, valid loss=70.416373\n",
      "[26000] train loss=68.196043, valid loss=70.416374\n",
      "[27000] train loss=68.196033, valid loss=70.416375\n",
      "[28000] train loss=68.196023, valid loss=70.416376\n",
      "[29000] train loss=68.196013, valid loss=70.416377\n",
      "[30000] train loss=68.196004, valid loss=70.416377\n",
      "[31000] train loss=68.195995, valid loss=70.416377\n",
      "[32000] train loss=68.195987, valid loss=70.416377\n",
      "[33000] train loss=68.195979, valid loss=70.416376\n",
      "[34000] train loss=68.195971, valid loss=70.416375\n",
      "[35000] train loss=68.195963, valid loss=70.416374\n",
      "[36000] train loss=68.195956, valid loss=70.416373\n",
      "[37000] train loss=68.195948, valid loss=70.416372\n",
      "[38000] train loss=68.195942, valid loss=70.416371\n",
      "[39000] train loss=68.195935, valid loss=70.416369\n",
      "[40000] train loss=68.195928, valid loss=70.416368\n",
      "[41000] train loss=68.195922, valid loss=70.416366\n",
      "[42000] train loss=68.195916, valid loss=70.416364\n",
      "[43000] train loss=68.195910, valid loss=70.416362\n",
      "[44000] train loss=68.195904, valid loss=70.416360\n",
      "[45000] train loss=68.195898, valid loss=70.416358\n",
      "[46000] train loss=68.195892, valid loss=70.416356\n",
      "[47000] train loss=68.195887, valid loss=70.416354\n",
      "[48000] train loss=68.195881, valid loss=70.416352\n",
      "[49000] train loss=68.195876, valid loss=70.416350\n",
      "[50000] train loss=68.195871, valid loss=70.416348\n",
      "[51000] train loss=68.195866, valid loss=70.416346\n",
      "[52000] train loss=68.195861, valid loss=70.416344\n",
      "[53000] train loss=68.195856, valid loss=70.416341\n",
      "[54000] train loss=68.195852, valid loss=70.416339\n",
      "[55000] train loss=68.195847, valid loss=70.416337\n",
      "[56000] train loss=68.195842, valid loss=70.416335\n",
      "[57000] train loss=68.195838, valid loss=70.416333\n",
      "[58000] train loss=68.195834, valid loss=70.416331\n",
      "[59000] train loss=68.195829, valid loss=70.416329\n",
      "[60000] train loss=68.195825, valid loss=70.416326\n",
      "[61000] train loss=68.195821, valid loss=70.416324\n",
      "[62000] train loss=68.195817, valid loss=70.416322\n",
      "[63000] train loss=68.195813, valid loss=70.416320\n",
      "[64000] train loss=68.195809, valid loss=70.416318\n",
      "[65000] train loss=68.195806, valid loss=70.416316\n",
      "[66000] train loss=68.195802, valid loss=70.416314\n",
      "[67000] train loss=68.195798, valid loss=70.416312\n",
      "[68000] train loss=68.195795, valid loss=70.416310\n",
      "[69000] train loss=68.195791, valid loss=70.416308\n",
      "[70000] train loss=68.195788, valid loss=70.416306\n",
      "[71000] train loss=68.195784, valid loss=70.416304\n",
      "[72000] train loss=68.195781, valid loss=70.416303\n",
      "[73000] train loss=68.195778, valid loss=70.416301\n",
      "[74000] train loss=68.195775, valid loss=70.416299\n",
      "[75000] train loss=68.195772, valid loss=70.416297\n",
      "[76000] train loss=68.195769, valid loss=70.416295\n",
      "[77000] train loss=68.195766, valid loss=70.416294\n",
      "[78000] train loss=68.195763, valid loss=70.416292\n",
      "[79000] train loss=68.195760, valid loss=70.416290\n",
      "[80000] train loss=68.195757, valid loss=70.416288\n",
      "[81000] train loss=68.195754, valid loss=70.416287\n",
      "[82000] train loss=68.195751, valid loss=70.416285\n",
      "[83000] train loss=68.195749, valid loss=70.416284\n",
      "[84000] train loss=68.195746, valid loss=70.416282\n",
      "[85000] train loss=68.195743, valid loss=70.416280\n",
      "[86000] train loss=68.195741, valid loss=70.416279\n",
      "[87000] train loss=68.195738, valid loss=70.416277\n",
      "[88000] train loss=68.195736, valid loss=70.416276\n",
      "[89000] train loss=68.195733, valid loss=70.416274\n",
      "[90000] train loss=68.195731, valid loss=70.416273\n",
      "[91000] train loss=68.195729, valid loss=70.416271\n",
      "[92000] train loss=68.195726, valid loss=70.416270\n",
      "[93000] train loss=68.195724, valid loss=70.416269\n",
      "[94000] train loss=68.195722, valid loss=70.416267\n",
      "[95000] train loss=68.195720, valid loss=70.416266\n",
      "[96000] train loss=68.195718, valid loss=70.416265\n",
      "[97000] train loss=68.195715, valid loss=70.416263\n",
      "[98000] train loss=68.195713, valid loss=70.416262\n",
      "[99000] train loss=68.195711, valid loss=70.416261\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "losst = 0\n",
    "lossv = 0\n",
    "for e in range(epoch):\n",
    "    \n",
    "    for bat in range(math.ceil(data_train.shape[0]/batch)):\n",
    "        front = bat*batch\n",
    "        end = (bat+1)*batch\n",
    "        if (end > data_train.shape[0]):\n",
    "            end = data_train.shape[0]\n",
    "        \n",
    "        data_batch = data_train[front:end, :]\n",
    "        y_ = np.matmul(data_batch, w)+b\n",
    "        \n",
    "        dif_y = y_-label_train[front:end]\n",
    "        dif_w = np.mean(data_batch*dif_y.reshape(dif_y.shape[0], 1), axis=0)\n",
    "        dif_b = dif_y.mean()\n",
    "        \n",
    "        # update\n",
    "        w = w-dif_w*rate_l\n",
    "        b = b-dif_b*rate_l\n",
    "    \n",
    "    # compute correct in train and valid\n",
    "    \n",
    "    # train\n",
    "    if (e%1000==0):\n",
    "        y_t = np.matmul(data_train, w)+b\n",
    "        losst = np.mean(np.square(y_t-label_train))\n",
    "\n",
    "        # valid\n",
    "        y_v = np.matmul(data_valid, w)+b\n",
    "        lossv = np.mean(np.square(y_v-label_valid))\n",
    "\n",
    "        print(\"[%d] train loss=%f, valid loss=%f\"\\\n",
    "              %(e, losst, lossv))\n",
    "        \n",
    "    if e%1000 == 0:\n",
    "        if lossv < 10:\n",
    "            rate_l = 0.001\n",
    "        elif lossv < 30:\n",
    "            rate_l = 0.002\n",
    "        elif lossv < 50:\n",
    "            rate_l = 0.003\n",
    "        elif lossv < 70:\n",
    "            rate_l = 0.004\n",
    "        elif lossv < 90:\n",
    "            rate_l = 0.005\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11250, 375)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
